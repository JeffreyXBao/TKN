# The @tf.function trace-compiles train_step into a TF graph for faster
# execution. The function specializes to the precise shape of the argument
# tensors. To avoid re-tracing due to the variable sequence lengths or variable
# batch sizes (the last batch is smaller), use input_signature to specify
# more generic shapes.

#TODO: FIX SIGNATURE STUFF

# train_step_signature = [
#     tf.TensorSpec(shape=(None, None), dtype=tf.float64),
#     tf.TensorSpec(shape=(None, None), dtype=tf.float64),
# ]

# @tf.function(input_signature=train_step_signature)
def train_step(inp, actual):
  with tf.GradientTape() as tape:
    prediction = model(inp, True)
    loss = loss_function(actual, prediction)

  gradients = tape.gradient(loss, model.trainable_variables)
  optimizer.apply_gradients(zip(gradients, model.trainable_variables))

  train_loss(loss)














  import math

  EPOCHS = 5

  #TODO IMPLEMENT BATCHES AND SHUFFLING

  for epoch in range(EPOCHS):
    start = time.time()

    train_loss.reset_states()

    batch_size = 100
    for i in range(0, math.ceil(len(X_train) / batch_size)):
      low = i*batch_size
      high = min(low + batch_size - 1, len(X_train))
      train_step(X_train[low:high], y_train[low:high])

      #if batch % 50 == 0:
      if i % 50 == 0:
        print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1, i, train_loss.result()))

    if (epoch + 1) % 5 == 0:
      ckpt_save_path = ckpt_manager.save()
      print ('Saving checkpoint for epoch {} at {}'.format(epoch+1, ckpt_save_path))

    print ('Epoch {} Loss {:.4f}'.format(epoch + 1, train_loss.result()))
    print ('Time taken for 1 epoch: {} secs\n'.format(time.time() - start))
